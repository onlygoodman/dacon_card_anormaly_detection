{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onlygoodman/dacon_card_anormaly_detection/blob/main/project_cfr_forgitgub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBTAnPQrEW7e"
      },
      "source": [
        "\"Isolation Forest\" 모델 사용\n",
        "001 feature 골라서 사용하면서 + pca 사용하여 최고점수 도출시도\n",
        "\n",
        "002 코사인, 피어슨 유사도 특성추가\n",
        "\n",
        "003 -\n",
        "\n",
        "004 Isolation Forest가 Robust모델에 비해 f1, auc 스코어 간의 격차가 적어 hyperparameter값 세부조정 시도\n",
        "\n",
        "- Isolation Forest h-params는 nes=150근처 pca는 사용하지 않을 때 성능이 가장 좋았다. -> 최대한 독립적인 특성만 사용하면서 학습시켰기 때문이라고 생각\n",
        "\n",
        "005 성능 개선을 위해서는 특성추가가 필요하다고 생각 11, 12, 17에서\n",
        "2, 4, 9, 10, 11, 12, 16, 17, 18 으로 특성 늘려서 학습\n",
        "\n",
        "006 4,9번특성이 성능이 잘나와서 4,9,11,12,17,31,32 특성을 사용하고 LOF와 OCS 모델 추가 사용하여 학습\n",
        "\n",
        "008 코사인유사도, 피어슨유사도, 11,12,17번을 사용하고 robust 모델의 성능이 개선되는 feature을 도출해내는 과정\n",
        "\n",
        "009 - 008의 실험결과가 14번이 생각이상으로 좋은 특성이었던것 같고 [29, 28, 27, 23, 21, 20, 8, 7, 5, 2]가 모델 성능을 현저히 떨어트리는 특성인것같다. 해당 특성을 제외하고 코사인, 피어슨 유사도를 측정 11,12,14,17,31,32 로 학습한 결과와 비교\n",
        "\n",
        "010 - 009에서 14번 추가 후 성능상승, 1번부터 다시 최적의 특성탐색\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlHtVtnQECWj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import permutations,combinations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "\n",
        "import math #차후 factorial 계산 등 수학적인 계산을 위한 import\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import PolynomialFeatures #특성공학을 사용하기 위한 import\n",
        "from sklearn.preprocessing import StandardScaler #전처리된 데이터들의 scale을 맞춰주기 위한 import\n",
        "from sklearn.decomposition import PCA #차원 축소 - 주성분의 개수에 따른 최적화 모델 구현을 위한 import\n",
        "\n",
        "\n",
        "\n",
        "#model\n",
        "from sklearn import svm\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.cluster import KMeans\n",
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score #이후 test.csv 파일의 predict 자료의 score을 매기기 위한 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRnMJVzsEIU6"
      },
      "outputs": [],
      "source": [
        "# Train dataset은 Label이 존재하지 않음\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/ai7/train.csv\")\n",
        "val_df  = pd.read_csv(\"/content/drive/MyDrive/ai7/val.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/ai7/test.csv\")\n",
        "submit = pd.read_csv(\"/content/drive/MyDrive/ai7/sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZJiZLavELjX",
        "outputId": "9e899e44-818b-4314-9a56-70f3f1c1d4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ID', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30']\n",
            "[0 0 0 ... 0 0 0]\n",
            "30 28432\n",
            "outliers_fraction = 0.0010540369615627855\n"
          ]
        }
      ],
      "source": [
        "clist = list(train_df.columns)\n",
        "print(clist)\n",
        "\n",
        "train_data = train_df[clist[1:]].to_numpy()\n",
        "val_data = val_df[clist[1:]].to_numpy()\n",
        "val_target = val_df['Class'].to_numpy()\n",
        "\n",
        "print(val_target)\n",
        "\n",
        "valcount1=0\n",
        "valcountm1=0\n",
        "for i in val_target:\n",
        "  if i==1:\n",
        "    valcount1+=1\n",
        "  else:\n",
        "    valcountm1+=1\n",
        "\n",
        "print(valcount1, valcountm1)\n",
        "\n",
        "outliers_fraction = valcount1/(valcount1+valcountm1)\n",
        "print(\"outliers_fraction =\",outliers_fraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etI6DUKsKUG0"
      },
      "source": [
        "코사인유사도 구하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtAsOqdALGZ5"
      },
      "outputs": [],
      "source": [
        "col = [clist[i] for i in range(1, 30+1) if i != 29] #유사도를 구할 때 사용 할 특성\n",
        "\n",
        "val_abnor = val_df[val_df['Class']==1][col]\n",
        "abnor_vector = val_abnor.mean().to_numpy()\n",
        "\n",
        "def cos_sim(a, b):\n",
        "  return dot(a, b)/(norm(a)*norm(b))\n",
        "\n",
        "def cosfeature(data):\n",
        "  datacs=[]\n",
        "  for i in data[col].to_numpy():\n",
        "    datacs.append(cos_sim(abnor_vector, i))\n",
        "  data['V31']=datacs\n",
        "\n",
        "cosfeature(train_df)\n",
        "cosfeature(val_df)\n",
        "cosfeature(test_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYm4ee1x3YTi"
      },
      "source": [
        "피어슨유사도 구하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ibefjok3aJP"
      },
      "outputs": [],
      "source": [
        "def pearson_similarity(a, b):\n",
        "    return np.dot((a - np.mean(a)), (b - np.mean(b))) / ((np.linalg.norm(a - np.mean(a))) * (np.linalg.norm(b - np.mean(b))))\n",
        "\n",
        "def pearsonfeature(data):\n",
        "  datacs=[]\n",
        "  for i in data[col].to_numpy():\n",
        "    datacs.append(pearson_similarity(abnor_vector, i))\n",
        "  data['V32']=datacs\n",
        "\n",
        "pearsonfeature(train_df)\n",
        "pearsonfeature(val_df)\n",
        "pearsonfeature(test_df)\n",
        "\n",
        "clist = list(train_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FrxVSnmPsS6",
        "outputId": "a1cf80b0-91b6-4d36-828c-4fe6969186d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n",
            "['ID', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32']\n"
          ]
        }
      ],
      "source": [
        "print(len(clist))\n",
        "print(clist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSwRkqMsEVcG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAk3cqMAc0Vb",
        "outputId": "22810622-7bd3-4940-cfc4-ac99b3c7de6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------\n",
            "!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\n",
            "----------------------------------------------------\n",
            "no.0001 model_RC\t----------- \tfeature:['V11', 'V12', 'V14', 'V17', 'V31'],k:0\t-----------max-0.9309641419574388\n",
            "pca-ncom:5 \n",
            " f1-score : 93.10000000000001,\t\tauc-score : 91.66\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def predata(data):\n",
        "  return data.drop(columns=['ID'])[col]\n",
        "\n",
        "def get_pred_label(model_pred):\n",
        "    # IsolationForest 모델 출력 (1:정상, -1:불량(사기)) 이므로 (0:정상, 1:불량(사기))로 Label 변환\n",
        "    model_pred = np.where(model_pred == 1, 0, model_pred)\n",
        "    model_pred = np.where(model_pred == -1, 1, model_pred)\n",
        "    return model_pred\n",
        "\n",
        "scoreboard={}\n",
        "maxscoreboard=[]\n",
        "modelscoreboard={\"IF\":0, \"RC\":0, \"LOF\":0, \"OCS\":0}\n",
        "th_score=0 #제출 파일 저장 문턱점수 \n",
        "maxscore=0 #지난회차 최고점수\n",
        "testname=\"010\"\n",
        "poly_dim =1\n",
        "testnum = 0\n",
        "\n",
        "mainfeature = [11, 12, 14, 17, 31]\n",
        "\n",
        "# for k in range(1, 4):\n",
        "  # for ctu in list(combinations([2,4,9,10,16,18], k)):\n",
        "      #  for ctu in [i for i in range(1,31) if i not in [11, 12, 17]]:  \n",
        "for k in range(1):\n",
        "  for ctu in range(1):  \n",
        "    col = [clist[i] for i in mainfeature]\n",
        "    \n",
        "    train_x = predata(train_df)\n",
        "    val_x = predata(val_df) # Input Data\n",
        "    val_y = val_df['Class'] # Label\n",
        "\n",
        "    test_x = predata(test_df)\n",
        "\n",
        "    for i in range(len(col), len(col)+1):\n",
        "      ss = StandardScaler()\n",
        "      ss.fit(train_x)\n",
        "      train_x = ss.transform(train_x)\n",
        "      val_x = ss.transform(val_x)\n",
        "      test_x = ss.transform(test_x)\n",
        "\n",
        "      pca = PCA(n_components = i)\n",
        "      pca.fit(train_x)\n",
        "      train_pca = pca.transform(train_x)\n",
        "      val_pca = pca.transform(val_x)\n",
        "      test_pca = pca.transform(test_x)\n",
        "\n",
        "      # anomaly_algorithms = [(\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction)),\n",
        "      #                     (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\",gamma=0.1)),\n",
        "      #                     (\"Isolation Forest\", IsolationForest(max_samples=len(train_pca), contamination=outliers_fraction,random_state=42)),\n",
        "      #                     (\"Local Outlier Factor\", LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction))]\n",
        "\n",
        "      contamination_controlnumber=0\n",
        "      \n",
        "      nes=150\n",
        "\n",
        "      ##lof hp\n",
        "      neig=35\n",
        "\n",
        "      ##ocs hp\n",
        "      gam=0.1\n",
        "      ker=\"rbf\"\n",
        "\n",
        "      model_if = IsolationForest(n_estimators=nes, max_samples=len(train_pca), contamination=outliers_fraction, random_state=42, verbose=0)\n",
        "      model_ocs = svm.OneClassSVM(nu=outliers_fraction, kernel=ker ,gamma=gam)\n",
        "      model_rc = EllipticEnvelope(contamination=outliers_fraction + contamination_controlnumber)\n",
        "      model_lof = LocalOutlierFactor(n_neighbors=neig, contamination=outliers_fraction)\n",
        "\n",
        "      # (\"One-Class SVM\",model_ocs) \n",
        "      # model_list=[(\"Isolation Forest\", model_if), (\"Robust covariance\", model_rc),(\"Local Outlier Factor\", model_lof),(\"One-Class SVM\", model_ocs)]\n",
        "      model_list=[(\"IF\", model_if), (\"RC\", model_rc)]#,(\"LOF\", model_lof),(\"OCS\", model_ocs)]\n",
        "      \n",
        "      for name, m in model_list[1:]:\n",
        "        \n",
        "        testnum+=1\n",
        "        m.fit(train_pca)\n",
        "\n",
        "        if name == \"LOF\":\n",
        "            val_pred = m.fit_predict(val_pca)\n",
        "        else:\n",
        "            val_pred = m.predict(val_pca)\n",
        "\n",
        "        val_pred = get_pred_label(val_pred)\n",
        "        val_score = f1_score(val_y, val_pred, average='macro')\n",
        "        val_auc_score = roc_auc_score(val_y, val_pred)\n",
        "        scoreboard[\"model:{}, pca-ncom:{}, contamination:{}\".format(name, i, outliers_fraction + contamination_controlnumber)]=\\\n",
        "        \"f1-score : {}, auc-score : {}\".format(round(val_score, 4)*100, round(val_auc_score, 4)*100)\n",
        "\n",
        "        if val_score >= modelscoreboard[name]-0.0000001:\n",
        "          maxscore = val_score\n",
        "          modelscoreboard[name] = val_score\n",
        "          maxscoreboard.append(\"no.{}-f1-score:{},\\t\\t auc-score:{}\".format(testnum, val_score, val_auc_score))\n",
        "          print(\"----------------------------------------------------\")\n",
        "          print(\"!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "          print(\"----------------------------------------------------\")\n",
        "\n",
        "          test_pred = m.predict(test_pca) # model prediction  \n",
        "          test_pred = get_pred_label(test_pred)\n",
        "          submit['Class'] = test_pred\n",
        "          submit.to_csv('./submit{}_no.{:0>4}_model_{}.csv'.format(testname, testnum, name), index=False)\n",
        "\n",
        "\n",
        "        print(\"no.{:0>4} model_{}\\t----------- \\tfeature:{},k:{}\\t-----------max-{}\".format(testnum, name, col, k,maxscore))\n",
        "\n",
        "        if name == model_list[0][0]:    #is\n",
        "          print('pca-ncom:{}, n-esti:{}'.format(i, nes),'\\n',\n",
        "              'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "          \n",
        "        elif name == model_list[1][0]:  #rb\n",
        "          print('pca-ncom:{}'.format(i),'\\n',\n",
        "              'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "          \n",
        "        # elif name == model_list[2][0]:  #lof\n",
        "        #   print('pca-ncom:{}, neig:{}'.format(i, neig),'\\n',\n",
        "        #       'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "          \n",
        "        # else:                           #ocs\n",
        "        #   print('pca-ncom:{}, ganma:{}, kenel:{}'.format(i, gam, ker),'\\n',\n",
        "        #       'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "          \n",
        "        # print(f'Validation F1 Score : [{val_score}]')\n",
        "        # print(classification_report(val_y, val_pred))\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def predata(data):\n",
        "#   return data.drop(columns=['ID'])[col]\n",
        "\n",
        "# def get_pred_label(model_pred):\n",
        "#     # IsolationForest 모델 출력 (1:정상, -1:불량(사기)) 이므로 (0:정상, 1:불량(사기))로 Label 변환\n",
        "#     model_pred = np.where(model_pred == 1, 0, model_pred)\n",
        "#     model_pred = np.where(model_pred == -1, 1, model_pred)\n",
        "#     return model_pred\n",
        "\n",
        "# scoreboard={}\n",
        "# maxscoreboard=[]\n",
        "# modelscoreboard={\"IF\":0, \"RC\":0, \"LOF\":0, \"OCS\":0}\n",
        "# th_score=0 #제출 파일 저장 문턱점수 \n",
        "# maxscore=0 #지난회차 최고점수\n",
        "# testname=\"010\"\n",
        "# poly_dim =1\n",
        "# testnum = 0\n",
        "\n",
        "# mainfeature = [11, 12, 14, 17, 31, 32]\n",
        "\n",
        "# # for k in range(1, 4):\n",
        "#   # for ctu in list(combinations([2,4,9,10,16,18], k)):\n",
        "#       #  for ctu in [i for i in range(1,31) if i not in [11, 12, 17]]:  \n",
        "# for k in [[3,10],[3,13]]:\n",
        "#   for j in [26,30]:\n",
        "#     for ctu in [x for x in range(1, 30+1) if x not in mainfeature+[3,10,13,15,22,26,29,30]]:  \n",
        "#       col = [clist[i] for i in mainfeature+k+[j]+[ctu]]\n",
        "      \n",
        "#       train_x = predata(train_df)\n",
        "#       val_x = predata(val_df) # Input Data\n",
        "#       val_y = val_df['Class'] # Label\n",
        "\n",
        "#       test_x = predata(test_df)\n",
        "\n",
        "#       for i in range(len(col), len(col)+1):\n",
        "#         ss = StandardScaler()\n",
        "#         ss.fit(train_x)\n",
        "#         train_x = ss.transform(train_x)\n",
        "#         val_x = ss.transform(val_x)\n",
        "#         test_x = ss.transform(test_x)\n",
        "\n",
        "#         pca = PCA(n_components = i)\n",
        "#         pca.fit(train_x)\n",
        "#         train_pca = pca.transform(train_x)\n",
        "#         val_pca = pca.transform(val_x)\n",
        "#         test_pca = pca.transform(test_x)\n",
        "\n",
        "#         # anomaly_algorithms = [(\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction)),\n",
        "#         #                     (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\",gamma=0.1)),\n",
        "#         #                     (\"Isolation Forest\", IsolationForest(max_samples=len(train_pca), contamination=outliers_fraction,random_state=42)),\n",
        "#         #                     (\"Local Outlier Factor\", LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction))]\n",
        "\n",
        "#         contamination_controlnumber=0\n",
        "        \n",
        "#         nes=150\n",
        "\n",
        "#         ##lof hp\n",
        "#         neig=35\n",
        "\n",
        "#         ##ocs hp\n",
        "#         gam=0.1\n",
        "#         ker=\"rbf\"\n",
        "\n",
        "#         model_if = IsolationForest(n_estimators=nes, max_samples=len(train_pca), contamination=outliers_fraction, random_state=42, verbose=0)\n",
        "#         model_ocs = svm.OneClassSVM(nu=outliers_fraction, kernel=ker ,gamma=gam)\n",
        "#         model_rc = EllipticEnvelope(contamination=outliers_fraction + contamination_controlnumber)\n",
        "#         model_lof = LocalOutlierFactor(n_neighbors=neig, contamination=outliers_fraction)\n",
        "\n",
        "#         # (\"One-Class SVM\",model_ocs) \n",
        "#         # model_list=[(\"Isolation Forest\", model_if), (\"Robust covariance\", model_rc),(\"Local Outlier Factor\", model_lof),(\"One-Class SVM\", model_ocs)]\n",
        "#         model_list=[(\"IF\", model_if), (\"RC\", model_rc)]#,(\"LOF\", model_lof),(\"OCS\", model_ocs)]\n",
        "        \n",
        "#         for name, m in model_list[1:]:\n",
        "          \n",
        "#           testnum+=1\n",
        "#           m.fit(train_pca)\n",
        "\n",
        "#           if name == \"LOF\":\n",
        "#               val_pred = m.fit_predict(val_pca)\n",
        "#           else:\n",
        "#               val_pred = m.predict(val_pca)\n",
        "\n",
        "#           val_pred = get_pred_label(val_pred)\n",
        "#           val_score = f1_score(val_y, val_pred, average='macro')\n",
        "#           val_auc_score = roc_auc_score(val_y, val_pred)\n",
        "#           scoreboard[\"model:{}, pca-ncom:{}, contamination:{}\".format(name, i, outliers_fraction + contamination_controlnumber)]=\\\n",
        "#           \"f1-score : {}, auc-score : {}\".format(round(val_score, 4)*100, round(val_auc_score, 4)*100)\n",
        "\n",
        "#           if val_score >= modelscoreboard[name]-0.0000001:\n",
        "#             maxscore = val_score\n",
        "#             modelscoreboard[name] = val_score\n",
        "#             maxscoreboard.append(\"no.{}-f1-score:{},\\t\\t auc-score:{}\".format(testnum, val_score, val_auc_score))\n",
        "#             print(\"----------------------------------------------------\")\n",
        "#             print(\"!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "#             print(\"----------------------------------------------------\")\n",
        "\n",
        "#             # test_pred = m.predict(test_pca) # model prediction  \n",
        "#             # test_pred = get_pred_label(test_pred)\n",
        "#             # submit['Class'] = test_pred\n",
        "#             # submit.to_csv('./submit{}_no.{:0>4}_model_{}.csv'.format(testname, testnum, name), index=False)\n",
        "\n",
        "\n",
        "#           print(\"no.{:0>4} model_{}\\t----------- \\tfeature:{},k:{}\\t-----------max-{}\".format(testnum, name, col, k,maxscore))\n",
        "\n",
        "#           if name == model_list[0][0]:    #is\n",
        "#             print('pca-ncom:{}, n-esti:{}'.format(i, nes),'\\n',\n",
        "#                 'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "            \n",
        "#           elif name == model_list[1][0]:  #rb\n",
        "#             print('pca-ncom:{}'.format(i),'\\n',\n",
        "#                 'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "            \n",
        "#           # elif name == model_list[2][0]:  #lof\n",
        "#           #   print('pca-ncom:{}, neig:{}'.format(i, neig),'\\n',\n",
        "#           #       'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "            \n",
        "#           # else:                           #ocs\n",
        "#           #   print('pca-ncom:{}, ganma:{}, kenel:{}'.format(i, gam, ker),'\\n',\n",
        "#           #       'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "            \n",
        "#           # print(f'Validation F1 Score : [{val_score}]')\n",
        "#           # print(classification_report(val_y, val_pred))\n",
        "#           print()"
      ],
      "metadata": {
        "id": "Me54AghDVxYE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10HB6j2nzydGMmB9cQO8QuW_F9UJpFgjx",
      "authorship_tag": "ABX9TyM8P+efxLRJTu5t9iEcQ5u4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}